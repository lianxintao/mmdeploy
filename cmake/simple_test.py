"""
ç®€å•æµ‹è¯•åˆå¹¶MoEå†…æ ¸
"""

import torch
import triton.language as tl
import sys
import os

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_kernel_basic():
    """
    åŸºæœ¬çš„kernelæµ‹è¯•
    """
    print("=== æµ‹è¯•åˆå¹¶MoEå†…æ ¸åŸºæœ¬åŠŸèƒ½ ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        from merged_moe_kernel import invoke_merged_moe_kernel
        from fused_moe import moe_align_block_size
        print("âœ… æˆåŠŸå¯¼å…¥æ¨¡å—")
    except Exception as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    device = "cuda"
    dtype = torch.float16
    
    # å°è§„æ¨¡æµ‹è¯•
    M, K, N1, N2, E, top_k = 32, 128, 256, 128, 4, 2
    
    try:
        # è¾“å…¥æ•°æ®
        A = torch.randn(M, K, device=device, dtype=dtype)
        W1 = torch.randn(E, N1, K, device=device, dtype=dtype)
        W2 = torch.randn(E, N2, N1 // 2, device=device, dtype=dtype)
        bias1 = torch.randn(E, N1, device=device, dtype=dtype)
        bias2 = torch.randn(E, N2, device=device, dtype=dtype)
        
        # è·¯ç”±ä¿¡æ¯
        topk_ids = torch.randint(0, E, (M, top_k), device=device, dtype=torch.int32)
        topk_weights = torch.rand(M, top_k, device=device, dtype=dtype)
        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
        
        print("âœ… æˆåŠŸåˆ›å»ºæµ‹è¯•æ•°æ®")
        
        # é…ç½®
        config = {
            "BLOCK_SIZE_M": 16,
            "BLOCK_SIZE_N": 32,
            "BLOCK_SIZE_K": 32,
            "GROUP_SIZE_M": 2,
            "num_warps": 2,
            "num_stages": 2,
        }
        
        # å¯¹é½å—å¤§å°
        sorted_token_ids, expert_ids, num_tokens_post_padded = moe_align_block_size(
            topk_ids, config["BLOCK_SIZE_M"], E
        )
        
        print("âœ… æˆåŠŸå®Œæˆå—å¯¹é½")
        
        # åˆ›å»ºè¾“å‡ºå¼ é‡
        C = torch.empty(M, top_k, N2, device=device, dtype=dtype)
        
        # è°ƒç”¨åˆå¹¶å†…æ ¸
        invoke_merged_moe_kernel(
            A=A,
            W1=W1,
            W2=W2,
            C=C,
            bias1=bias1,
            bias2=bias2,
            topk_weights=topk_weights,
            topk_ids=topk_ids,
            sorted_token_ids=sorted_token_ids,
            expert_ids=expert_ids,
            num_tokens_post_padded=num_tokens_post_padded,
            mul_routed_weight=False,  # å…ˆæµ‹è¯•ä¸åº”ç”¨è·¯ç”±æƒé‡
            top_k=top_k,
            config=config,
            compute_type=tl.float16,
        )
        
        print("âœ… æˆåŠŸæ‰§è¡Œåˆå¹¶å†…æ ¸")
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(C).any():
            print("âŒ è¾“å‡ºåŒ…å«NaN")
            return False
        
        if torch.isinf(C).any():
            print("âŒ è¾“å‡ºåŒ…å«Inf")
            return False
        
        print(f"âœ… è¾“å‡ºå½¢çŠ¶: {C.shape}")
        print(f"âœ… è¾“å‡ºèŒƒå›´: [{C.min().item():.4f}, {C.max().item():.4f}]")
        print(f"âœ… è¾“å‡ºå‡å€¼: {C.mean().item():.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å†…æ ¸æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_with_reference():
    """
    ä¸å‚è€ƒå®ç°å¯¹æ¯”æµ‹è¯•
    """
    print("\n=== ä¸å‚è€ƒå®ç°å¯¹æ¯”æµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        from merged_moe_kernel import invoke_merged_moe_kernel
        from fused_moe import invoke_fused_moe_kernel, moe_align_block_size
        print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰æ¨¡å—")
    except Exception as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    device = "cuda"
    dtype = torch.float16
    
    M, K, N1, N2, E, top_k = 16, 64, 128, 64, 2, 1  # éå¸¸å°çš„è§„æ¨¡
    
    try:
        # è¾“å…¥æ•°æ®
        A = torch.randn(M, K, device=device, dtype=dtype)
        W1 = torch.randn(E, N1, K, device=device, dtype=dtype)
        W2 = torch.randn(E, N2, N1 // 2, device=device, dtype=dtype)
        bias1 = torch.randn(E, N1, device=device, dtype=dtype)
        bias2 = torch.randn(E, N2, device=device, dtype=dtype)
        
        # è·¯ç”±ä¿¡æ¯
        topk_ids = torch.randint(0, E, (M, top_k), device=device, dtype=torch.int32)
        topk_weights = torch.ones(M, top_k, device=device, dtype=dtype)  # å…¨ä¸º1ï¼Œç®€åŒ–æµ‹è¯•
        
        print("âœ… æˆåŠŸåˆ›å»ºæµ‹è¯•æ•°æ®")
        
        # é…ç½®
        config = {
            "BLOCK_SIZE_M": 16,
            "BLOCK_SIZE_N": 32,
            "BLOCK_SIZE_K": 32,
            "GROUP_SIZE_M": 2,
            "num_warps": 2,
            "num_stages": 2,
        }
        
        # å¯¹é½å—å¤§å°
        sorted_token_ids, expert_ids, num_tokens_post_padded = moe_align_block_size(
            topk_ids, config["BLOCK_SIZE_M"], E
        )
        
        # ======= å‚è€ƒå®ç°ï¼šä¸‰æ­¥æ³• =======
        print("è¿è¡Œå‚è€ƒå®ç°ï¼ˆä¸‰æ­¥æ³•ï¼‰...")
        
        # ç¬¬ä¸€æ­¥ï¼šA @ W1 + bias1
        intermediate1 = torch.empty(M, top_k, N1, device=device, dtype=dtype)
        invoke_fused_moe_kernel(
            A, W1, bias1, intermediate1,
            None, None, None,  # é‡åŒ–ç›¸å…³å‚æ•°
            topk_weights, topk_ids,
            sorted_token_ids, expert_ids, num_tokens_post_padded,
            False,  # mul_routed_weight
            top_k, config, tl.float16,
            False, False, False, False, False,  # é‡åŒ–æ ‡å¿—
            None  # block_shape
        )
        
        # ç¬¬äºŒæ­¥ï¼šSiLUå’Œé—¨æ§
        # æ¨¡æ‹Ÿsilu_and_mulçš„åŠŸèƒ½
        intermediate1_reshaped = intermediate1.view(-1, N1)
        gate = intermediate1_reshaped[:, :N1//2]
        up = intermediate1_reshaped[:, N1//2:]
        gate_silu = gate / (1.0 + torch.exp(-gate))
        intermediate2 = (gate_silu * up).view(M, top_k, N1//2)
        
        # ç¬¬ä¸‰æ­¥ï¼šintermediate2 @ W2 + bias2
        reference_output = torch.empty(M, top_k, N2, device=device, dtype=dtype)
        invoke_fused_moe_kernel(
            intermediate2.view(M * top_k, N1//2), W2, bias2, reference_output,
            None, None, None,  # é‡åŒ–ç›¸å…³å‚æ•°
            topk_weights, topk_ids,
            sorted_token_ids, expert_ids, num_tokens_post_padded,
            False,  # mul_routed_weight
            1, config, tl.float16,  # æ³¨æ„ï¼šè¿™é‡Œtop_k=1å› ä¸ºå·²ç»æ˜¯å±•å¼€çš„
            False, False, False, False, False,  # é‡åŒ–æ ‡å¿—
            None  # block_shape
        )
        
        print("âœ… å‚è€ƒå®ç°å®Œæˆ")
        
        # ======= åˆå¹¶å†…æ ¸å®ç° =======
        print("è¿è¡Œåˆå¹¶å†…æ ¸å®ç°...")
        
        merged_output = torch.empty(M, top_k, N2, device=device, dtype=dtype)
        invoke_merged_moe_kernel(
            A=A,
            W1=W1,
            W2=W2,
            C=merged_output,
            bias1=bias1,
            bias2=bias2,
            topk_weights=topk_weights,
            topk_ids=topk_ids,
            sorted_token_ids=sorted_token_ids,
            expert_ids=expert_ids,
            num_tokens_post_padded=num_tokens_post_padded,
            mul_routed_weight=False,
            top_k=top_k,
            config=config,
            compute_type=tl.float16,
        )
        
        print("âœ… åˆå¹¶å†…æ ¸å®Œæˆ")
        
        # æ¯”è¾ƒç»“æœ
        diff = torch.abs(reference_output - merged_output)
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print(f"æœ€å¤§å·®å¼‚: {max_diff:.6f}")
        print(f"å¹³å‡å·®å¼‚: {mean_diff:.6f}")
        
        # ç›¸å¯¹è¯¯å·®
        relative_error = diff / (torch.abs(reference_output) + 1e-8)
        max_rel_error = relative_error.max().item()
        mean_rel_error = relative_error.mean().item()
        
        print(f"æœ€å¤§ç›¸å¯¹è¯¯å·®: {max_rel_error:.6f}")
        print(f"å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_rel_error:.6f}")
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        tolerance = 1e-2  # ç›¸å¯¹å®½æ¾çš„é˜ˆå€¼
        if max_rel_error < tolerance:
            print("âœ… å¯¹æ¯”æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            print("âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥ï¼")
            print(f"å‚è€ƒè¾“å‡ºæ ·æœ¬: {reference_output[0, 0, :5]}")
            print(f"åˆå¹¶è¾“å‡ºæ ·æœ¬: {merged_output[0, 0, :5]}")
            return False
            
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    """
    print("å¼€å§‹æµ‹è¯•åˆå¹¶MoEå†…æ ¸...\n")
    
    success = True
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    success &= test_kernel_basic()
    
    # ä¸å‚è€ƒå®ç°å¯¹æ¯”æµ‹è¯•
    success &= test_with_reference()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
    
    return success


if __name__ == "__main__":
    main()
