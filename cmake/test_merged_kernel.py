"""
æµ‹è¯•é«˜æ•ˆçš„ FP8 ä¸“ç”¨ MoE å†…æ ¸
"""

import torch
import triton
import sys
import os

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def create_efficient_fp8_test_data(
    M: int = 64,            # tokenæ•°é‡
    K: int = 256,           # è¾“å…¥ç»´åº¦
    N1: int = 512,          # ä¸­é—´ç»´åº¦
    N2: int = 256,          # è¾“å‡ºç»´åº¦
    E: int = 4,             # ä¸“å®¶æ•°é‡
    top_k: int = 1,         # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°
    device: str = "cuda",
    block_shape: list = [64, 64],
):
    """
    åˆ›å»ºé«˜æ•ˆ FP8 æµ‹è¯•æ•°æ®
    """
    block_n, block_k = block_shape
    
    # è¾“å…¥æ¿€æ´» (FP16ï¼Œä¼šåœ¨å†…æ ¸ä¸­é‡åŒ–ä¸ºFP8)
    hidden_states = torch.randn(M, K, device=device, dtype=torch.float16)
    
    # ä¸“å®¶æƒé‡ (FP8)
    w1 = torch.randn(E, N1, K, device=device, dtype=torch.float8_e4m3fn)
    w2 = torch.randn(E, N2, N1 // 2, device=device, dtype=torch.float8_e4m3fn)
    
    # åç½®
    b1 = torch.randn(E, N1, device=device, dtype=torch.float16)
    b2 = torch.randn(E, N2, device=device, dtype=torch.float16)
    
    # FP8 å—é‡åŒ–ç¼©æ”¾å› å­
    num_blocks_n1 = triton.cdiv(N1, block_n)
    num_blocks_k1 = triton.cdiv(K, block_k)
    num_blocks_n2 = triton.cdiv(N2, block_n)
    num_blocks_k2 = triton.cdiv(N1 // 2, block_k)
    
    w1_scale = torch.rand(E, num_blocks_n1, num_blocks_k1, device=device, dtype=torch.float32) * 0.1
    w2_scale = torch.rand(E, num_blocks_n2, num_blocks_k2, device=device, dtype=torch.float32) * 0.1
    a1_scale = torch.rand(M, num_blocks_k1, device=device, dtype=torch.float32) * 0.1
    a2_scale = torch.rand(M, num_blocks_k2, device=device, dtype=torch.float32) * 0.1
    
    # è·¯ç”±ä¿¡æ¯
    topk_ids = torch.randint(0, E, (M, top_k), device=device, dtype=torch.int32)
    topk_weights = torch.ones(M, top_k, device=device, dtype=torch.float16)
    
    return {
        'hidden_states': hidden_states,
        'w1': w1, 'w2': w2, 'b1': b1, 'b2': b2,
        'w1_scale': w1_scale, 'w2_scale': w2_scale,
        'a1_scale': a1_scale, 'a2_scale': a2_scale,
        'topk_weights': topk_weights, 'topk_ids': topk_ids,
    }


def test_efficient_fp8_kernel():
    """
    æµ‹è¯•é«˜æ•ˆ FP8 ä¸“ç”¨å†…æ ¸
    """
    print("=== æµ‹è¯•é«˜æ•ˆ FP8 ä¸“ç”¨ MoE å†…æ ¸ ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        from fp8_moe_kernel import fp8_moe_impl
        print("âœ… æˆåŠŸå¯¼å…¥é«˜æ•ˆ FP8 å†…æ ¸")
    except Exception as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    M, K, N1, N2, E, top_k = 32, 128, 256, 128, 4, 1
    block_shape = [64, 64]  # block_n, block_k
    
    test_data = create_efficient_fp8_test_data(
        M, K, N1, N2, E, top_k,
        block_shape=block_shape
    )
    
    try:
        # æµ‹è¯•é«˜æ•ˆ FP8 å†…æ ¸
        result = fp8_moe_impl(
            hidden_states=test_data['hidden_states'],
            w1=test_data['w1'],
            w2=test_data['w2'],
            topk_weights=test_data['topk_weights'],
            topk_ids=test_data['topk_ids'],
            b1=test_data['b1'],
            b2=test_data['b2'],
            w1_scale=test_data['w1_scale'],
            w2_scale=test_data['w2_scale'],
            a1_scale=test_data['a1_scale'],
            a2_scale=test_data['a2_scale'],
            block_shape=block_shape,
            inplace=False,
            activation="silu",
            apply_router_weight_on_input=False,
        )
        
        print("âœ… é«˜æ•ˆ FP8 å†…æ ¸æµ‹è¯•é€šè¿‡")
        print(f"è¾“å‡ºå½¢çŠ¶: {result.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{result.min().item():.4f}, {result.max().item():.4f}]")
        
        # æ£€æŸ¥è¾“å‡ºæœ‰æ•ˆæ€§
        if torch.isnan(result).any():
            print("âŒ è¾“å‡ºåŒ…å«NaN")
            return False
        
        if torch.isinf(result).any():
            print("âŒ è¾“å‡ºåŒ…å«Inf")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ é«˜æ•ˆ FP8 å†…æ ¸æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_performance_comparison():
    """
    æµ‹è¯•æ€§èƒ½å¯¹æ¯”
    """
    print("\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        from fp8_moe_kernel import fp8_moe_impl
        print("âœ… æˆåŠŸå¯¼å…¥é«˜æ•ˆ FP8 å†…æ ¸")
    except Exception as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºè¾ƒå¤§çš„æµ‹è¯•æ•°æ®è¿›è¡Œæ€§èƒ½æµ‹è¯•
    M, K, N1, N2, E, top_k = 256, 512, 1024, 512, 8, 2
    block_shape = [64, 64]
    
    test_data = create_efficient_fp8_test_data(
        M, K, N1, N2, E, top_k,
        block_shape=block_shape
    )
    
    try:
        # é¢„çƒ­
        for _ in range(3):
            result = fp8_moe_impl(
                hidden_states=test_data['hidden_states'],
                w1=test_data['w1'],
                w2=test_data['w2'],
                topk_weights=test_data['topk_weights'],
                topk_ids=test_data['topk_ids'],
                b1=test_data['b1'],
                b2=test_data['b2'],
                w1_scale=test_data['w1_scale'],
                w2_scale=test_data['w2_scale'],
                a1_scale=test_data['a1_scale'],
                a2_scale=test_data['a2_scale'],
                block_shape=block_shape,
            )
        
        # æ€§èƒ½æµ‹è¯•
        torch.cuda.synchronize()
        import time
        
        num_runs = 10
        start_time = time.time()
        
        for _ in range(num_runs):
            result = fp8_moe_impl(
                hidden_states=test_data['hidden_states'],
                w1=test_data['w1'],
                w2=test_data['w2'],
                topk_weights=test_data['topk_weights'],
                topk_ids=test_data['topk_ids'],
                b1=test_data['b1'],
                b2=test_data['b2'],
                w1_scale=test_data['w1_scale'],
                w2_scale=test_data['w2_scale'],
                a1_scale=test_data['a1_scale'],
                a2_scale=test_data['a2_scale'],
                block_shape=block_shape,
            )
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs * 1000  # ms
        print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_time:.2f} ms")
        print(f"å½¢çŠ¶: M={M}, K={K}, N1={N1}, N2={N2}, E={E}, top_k={top_k}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_different_sizes():
    """
    æµ‹è¯•ä¸åŒå°ºå¯¸é…ç½®
    """
    print("\n=== æµ‹è¯•ä¸åŒå°ºå¯¸é…ç½® ===")
    
    configurations = [
        {"name": "å°è§„æ¨¡", "M": 16, "K": 64, "N1": 128, "N2": 64, "E": 2, "top_k": 1},
        {"name": "ä¸­ç­‰è§„æ¨¡", "M": 64, "K": 256, "N1": 512, "N2": 256, "E": 4, "top_k": 2},
        {"name": "å¤§è§„æ¨¡", "M": 128, "K": 512, "N1": 1024, "N2": 512, "E": 8, "top_k": 2},
    ]
    
    success_count = 0
    
    for config in configurations:
        print(f"\næµ‹è¯•é…ç½®: {config['name']}")
        
        try:
            from fp8_moe_kernel import fp8_moe_impl
            
            block_shape = [64, 64]
            test_data = create_efficient_fp8_test_data(
                config["M"], config["K"], config["N1"], config["N2"], 
                config["E"], config["top_k"], block_shape=block_shape
            )
            
            result = fp8_moe_impl(
                hidden_states=test_data['hidden_states'],
                w1=test_data['w1'],
                w2=test_data['w2'],
                topk_weights=test_data['topk_weights'],
                topk_ids=test_data['topk_ids'],
                b1=test_data['b1'],
                b2=test_data['b2'],
                w1_scale=test_data['w1_scale'],
                w2_scale=test_data['w2_scale'],
                a1_scale=test_data['a1_scale'],
                a2_scale=test_data['a2_scale'],
                block_shape=block_shape,
            )
            
            expected_shape = (config["M"], config["N2"])
            if result.shape == expected_shape:
                print(f"âœ… {config['name']} æµ‹è¯•é€šè¿‡")
                success_count += 1
            else:
                print(f"âŒ {config['name']} å½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å¾—åˆ° {result.shape}")
                
        except Exception as e:
            print(f"âŒ {config['name']} æµ‹è¯•å¤±è´¥: {e}")
    
    print(f"\né…ç½®æµ‹è¯•ç»“æœ: {success_count}/{len(configurations)} é€šè¿‡")
    return success_count == len(configurations)


def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    """
    print("å¼€å§‹æµ‹è¯•é«˜æ•ˆ FP8 ä¸“ç”¨ MoE å†…æ ¸...\n")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ‰€æœ‰æµ‹è¯•")
        return
    
    success = True
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    success &= test_efficient_fp8_kernel()
    
    # æ€§èƒ½æµ‹è¯•
    success &= test_performance_comparison()
    
    # ä¸åŒé…ç½®æµ‹è¯•
    success &= test_different_sizes()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰é«˜æ•ˆ FP8 å†…æ ¸æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†é«˜æ•ˆ FP8 å†…æ ¸æµ‹è¯•å¤±è´¥")
    
    print("\nğŸ“ é«˜æ•ˆ FP8 å†…æ ¸ç‰¹æ€§:")
    print("  âœ… åˆ†å—è®¡ç®— - æ¯æ¬¡å¤„ç† (BLOCK_SIZE_M, BLOCK_SIZE_N) çš„è¾“å‡º")
    print("  âœ… FP8 ä¸“ç”¨ - æ— éé‡åŒ–åˆ†æ”¯ï¼Œæ€§èƒ½æœ€ä¼˜")
    print("  âœ… å—çº§é‡åŒ– - æœ€é«˜ç²¾åº¦çš„é‡åŒ–æ–¹æ¡ˆ")
    print("  âœ… å†…å­˜é«˜æ•ˆ - é¿å…å¤§ä¸­é—´çŸ©é˜µï¼Œå‡å°‘cache miss")
    print("  âœ… SiLUèåˆ - åœ¨åˆ†å—ä¸­ç›´æ¥è®¡ç®—æ¿€æ´»å’Œé—¨æ§")


if __name__ == "__main__":
    main()
