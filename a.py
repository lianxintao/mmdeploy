42.194.203.117:21116
42.194.203.117:21117
+Fa1jUWPsB2QxKC4+Ivk51ZswxliJjtWNTTPZY7J0tU=

110.40.200.222:21116
110.40.200.222:21117
ancFyyI87muqvtIvO8rJziPkOJSWs1Dq1kTroXtTFqo=
https://triton-lang.cn/main/getting-started/tutorials/index.html

@torch.no_grad()
def event_loop_pp_disagg_prefill(self: Scheduler) -> None:
    """A pipeline parallel scheduler loop for prefill worker in disaggregation mode."""
    
    # Initialize microbatch structures for pipeline parallelism
    mbs = [None] * self.pp_size
    last_mbs = [None] * self.pp_size
    self.running_mbs = [
        ScheduleBatch(reqs=[], batch_is_full=False) for _ in range(self.pp_size)
    ]
    bids = [None] * self.pp_size
    pp_outputs: Optional[PPProxyTensors] = None
    
    while True:
        server_is_idle = True
        
        for mb_id in range(self.pp_size):
            self.running_batch = self.running_mbs[mb_id]
            self.last_batch = last_mbs[mb_id]

            # Process incoming requests - similar to disagg_prefill
            recv_reqs = self.recv_requests()
            self.process_input_requests(recv_reqs)
            
            # Handle bootstrapped requests from disagg queue
            self.waiting_queue.extend(
                self.disagg_prefill_bootstrap_queue.pop_bootstrapped()
            )
            
            # Process prefill chunks for current microbatch
            self.process_prefill_chunk()
            
            # Get new batch for prefill processing
            mbs[mb_id] = self.get_new_batch_prefill()
            
            # Handle MLP sync if required
            if require_mlp_sync(self.server_args):
                mbs[mb_id], _ = self.prepare_mlp_sync_batch(mbs[mb_id])
                
            self.running_mbs[mb_id] = self.running_batch
            self.cur_batch = mbs[mb_id]
            
            if self.cur_batch:
                server_is_idle = False
                result = self.run_batch(self.cur_batch)

                # (last rank) send the outputs to the next step
                if self.pp_group.is_last_rank:
                    next_token_ids, bids[mb_id] = (
                        result.next_token_ids,
                        result.bid,
                    )
                    if self.cur_batch.return_logprob:
                        pp_outputs = PPProxyTensors(
                            {
                                "next_token_ids": next_token_ids,
                                "extend_input_len_per_req": result.extend_input_len_per_req,
                                "extend_logprob_start_len_per_req": result.extend_logprob_start_len_per_req,
                            }
                            | (
                                {
                                    f"logits_output.{k}": v
                                    for k, v in result.logits_output.__dict__.items()
                                }
                                if result.logits_output is not None
                                else {}
                            )
                        )
                    else:
                        pp_outputs = PPProxyTensors(
                            {
                                "next_token_ids": next_token_ids,
                            }
                        )
                    # send the output from the last round to let the next stage worker run post processing
                    self.pp_group.send_tensor_dict(
                        pp_outputs.tensors,
                        all_gather_group=self.attn_tp_group,
                    )

            # receive outputs and post-process (filter finished reqs) the coming microbatch
            next_mb_id = (mb_id + 1) % self.pp_size
            next_pp_outputs = None
            if mbs[next_mb_id] is not None:
                next_pp_outputs: Optional[PPProxyTensors] = PPProxyTensors(
                    self.pp_group.recv_tensor_dict(
                        all_gather_group=self.attn_tp_group
                    )
                )
                mbs[next_mb_id].output_ids = next_pp_outputs["next_token_ids"]
                logits_output_args = {
                    k[len("logits_output.") :]: v
                    for k, v in next_pp_outputs.tensors.items()
                    if k.startswith("logits_output.")
                }
                if len(logits_output_args) > 0:
                    logits_output = LogitsProcessorOutput(**logits_output_args)
                else:
                    logits_output = None
                output_result = GenerationBatchResult(
                    logits_output=logits_output,
                    pp_hidden_states_proxy_tensors=None,
                    next_token_ids=next_pp_outputs["next_token_ids"],
                    extend_input_len_per_req=next_pp_outputs.tensors.get(
                        "extend_input_len_per_req", None
                    ),
                    extend_logprob_start_len_per_req=next_pp_outputs.tensors.get(
                        "extend_logprob_start_len_per_req", None
                    ),
                    bid=bids[next_mb_id],
                    can_run_cuda_graph=result.can_run_cuda_graph,
                )
                # Use disagg_prefill specific batch result processing
                self.process_batch_result_disagg_prefill(mbs[next_mb_id], output_result)
                last_mbs[next_mb_id] = mbs[next_mb_id]

            # (not last rank)
            if not self.pp_group.is_last_rank:
                if self.cur_batch:
                    bids[mb_id] = result.bid
                # carry the outputs to the next stage
                # send the outputs from the last round to let the next stage worker run post processing
                if pp_outputs:
                    self.pp_group.send_tensor_dict(
                        pp_outputs.tensors,
                        all_gather_group=self.attn_tp_group,
                    )

                # send out reqs to the next stage
                dp_offset = self.attn_dp_rank * self.attn_tp_size
                if self.attn_tp_rank == 0:
                    point_to_point_pyobj(
                        recv_reqs,
                        self.pp_rank * self.tp_size + dp_offset,
                        self.world_group.cpu_group,
                        self.pp_rank * self.tp_size + dp_offset,
                        (self.pp_rank + 1) * self.tp_size + dp_offset,
                    )

                # send out proxy tensors to the next stage
                if self.cur_batch:
                    self.pp_group.send_tensor_dict(
                        result.pp_hidden_states_proxy_tensors,
                        all_gather_group=self.attn_tp_group,
                    )

            pp_outputs = next_pp_outputs
            
            # Process inflight disagg prefill queue for current microbatch
            if len(self.disagg_prefill_inflight_queue) > 0:
                self.process_disagg_prefill_inflight_queue()
            
            # Reset batch_is_full flag for each microbatch to prevent hanging under high concurrency
            # This is critical for PD separation mode
            self.running_mbs[mb_id].batch_is_full = False

        # When the server is idle, self-check and re-init some states
        # Only do this when all microbatches are None and inflight queue is empty
        all_mbs_none = all(mb is None for mb in mbs)
        if server_is_idle and all_mbs_none and len(self.disagg_prefill_inflight_queue) == 0:
            self.check_memory()
            self.new_token_ratio = self.init_new_token_ratio
            self.maybe_sleep_on_idle()


from sglang.srt.model_executor.logits_processor import LogitsProcessorOutput
from sglang.srt.torch_memory_saver_adapter import TorchMemorySaverAdapter
from sglang.srt.utils import require_mlp_sync, point_to_point_pyobj
from sglang.srt.managers.scheduler import PPProxyTensors



   @torch.no_grad()
    def event_loop_pp_disagg_decode(self: Scheduler):
        """A pipeline parallelism scheduler loop for decode worker in disaggregation mode."""
        mbs = [None] * self.pp_size
        last_mbs = [None] * self.pp_size
        self.running_mbs = [
            ScheduleBatch(reqs=[], batch_is_full=False) for _ in range(self.pp_size)
        ]
        bids = [None] * self.pp_size
        pp_outputs: Optional[PPProxyTensors] = None
        
        while True:
            server_is_idle = True
            for mb_id in range(self.pp_size):
                self.running_batch = self.running_mbs[mb_id]
                self.last_batch = last_mbs[mb_id]

                recv_reqs = self.recv_requests()
                self.process_input_requests(recv_reqs)
                # polling and allocating kv cache
                self.process_decode_queue()
                mbs[mb_id] = self.get_next_disagg_decode_batch_to_run()
                self.running_mbs[mb_id] = self.running_batch

                self.cur_batch = mbs[mb_id]
                prepare_mlp_sync_flag = require_mlp_sync(self.server_args)

                if self.cur_batch:
                    server_is_idle = False
                    # Generate fake extend output.
                    if self.cur_batch.forward_mode.is_extend():
                        # Note: Logprobs should be handled on the prefill engine.
                        self.stream_output(
                            self.cur_batch.reqs, any(req.return_logprob for req in self.cur_batch.reqs)
                        )
                        if prepare_mlp_sync_flag:
                            self._prepare_idle_batch_and_run(None)
                    else:
                        if prepare_mlp_sync_flag:
                            self.prepare_mlp_sync_batch(self.cur_batch)
                        result = self.run_batch(self.cur_batch)

                # (last rank) send the outputs to the next step
                if self.pp_group.is_last_rank:
                    if self.cur_batch and not self.cur_batch.forward_mode.is_extend():
                        next_token_ids, bids[mb_id] = (
                            result.next_token_ids,
                            result.bid,
                        )
                        if self.cur_batch.return_logprob:
                            pp_outputs = PPProxyTensors(
                                {
                                    "next_token_ids": next_token_ids,
                                    "extend_input_len_per_req": result.extend_input_len_per_req,
                                    "extend_logprob_start_len_per_req": result.extend_logprob_start_len_per_req,
                                }
                                | (
                                    {
                                        f"logits_output.{k}": v
                                        for k, v in result.logits_output.__dict__.items()
                                    }
                                    if result.logits_output is not None
                                    else {}
                                )
                            )
                        else:
                            pp_outputs = PPProxyTensors(
                                {
                                    "next_token_ids": next_token_ids,
                                }
                            )
                        # send the output from the last round to let the next stage worker run post processing
                        self.pp_group.send_tensor_dict(
                            pp_outputs.tensors,
                            all_gather_group=self.attn_tp_group,
                        )

                # receive outputs and post-process (filter finished reqs) the coming microbatch
                next_mb_id = (mb_id + 1) % self.pp_size
                next_pp_outputs = None
                if mbs[next_mb_id] is not None:
                    next_pp_outputs: Optional[PPProxyTensors] = PPProxyTensors(
                        self.pp_group.recv_tensor_dict(
                            all_gather_group=self.attn_tp_group
                        )
                    )
                    mbs[next_mb_id].output_ids = next_pp_outputs["next_token_ids"]
                    logits_output_args = {
                        k[len("logits_output.") :]: v
                        for k, v in next_pp_outputs.tensors.items()
                        if k.startswith("logits_output.")
                    }
                    if len(logits_output_args) > 0:
                        logits_output = LogitsProcessorOutput(**logits_output_args)
                    else:
                        logits_output = None
                    output_result = GenerationBatchResult(
                        logits_output=logits_output,
                        pp_hidden_states_proxy_tensors=None,
                        next_token_ids=next_pp_outputs["next_token_ids"],
                        extend_input_len_per_req=next_pp_outputs.tensors.get(
                            "extend_input_len_per_req", None
                        ),
                        extend_logprob_start_len_per_req=next_pp_outputs.tensors.get(
                            "extend_logprob_start_len_per_req", None
                        ),
                        bid=bids[next_mb_id],
                        can_run_cuda_graph=result.can_run_cuda_graph,
                    )
                    self.process_batch_result(mbs[next_mb_id], output_result)
                    last_mbs[next_mb_id] = mbs[next_mb_id]

                # (not last rank)
                if not self.pp_group.is_last_rank:
                    if self.cur_batch and not self.cur_batch.forward_mode.is_extend():
                        bids[mb_id] = result.bid
                    # carry the outputs to the next stage
                    # send the outputs from the last round to let the next stage worker run post processing
                    if pp_outputs:
                        self.pp_group.send_tensor_dict(
                            pp_outputs.tensors,
                            all_gather_group=self.attn_tp_group,
                        )

                    # send out reqs to the next stage
                    dp_offset = self.attn_dp_rank * self.attn_tp_size
                    if self.attn_tp_rank == 0:
                        point_to_point_pyobj(
                            recv_reqs,
                            self.pp_rank * self.tp_size + dp_offset,
                            self.world_group.cpu_group,
                            self.pp_rank * self.tp_size + dp_offset,
                            (self.pp_rank + 1) * self.tp_size + dp_offset,
                        )

                    # send out proxy tensors to the next stage
                    if self.cur_batch and not self.cur_batch.forward_mode.is_extend():
                        self.pp_group.send_tensor_dict(
                            result.pp_hidden_states_proxy_tensors,
                            all_gather_group=self.attn_tp_group,
                        )

                pp_outputs = next_pp_outputs

            # When the server is idle, self-check and re-init some states
            if server_is_idle and (
                len(self.waiting_queue)
                + len(self.disagg_decode_transfer_queue.queue)
                + len(self.disagg_decode_prealloc_queue.queue)
                == 0
            ):
                self.check_memory()
                self.new_token_ratio = self.init_new_token_ratio
                self.maybe_sleep_on_idle()
Áé∞Âú®ÁöÑdisa
        logger.info(
            f"decode rank: engine_rank={self.kv_mgr.kv_args.engine_rank}, "
            f"target_tp_ranks={self.target_tp_ranks}, target_dp_group={self.target_dp_group}, "
            f"bootstrap_addr={self.bootstrap_addr}"
        )
ggregation ‰∏çÊîØÊåÅ pipline Âπ∂Ë°åÔºåÂéüÂõ†ÊòØÂú®prefillÔºådecode ÁöÑÊñá‰ª∂‰∏≠Âπ∂Êú™ÂÉèschedulerÊñá‰ª∂‰∏≠ÈÇ£Ê†∑ÂçïÁã¨ÂÆûÁé∞‰∫ÜÈÄÇÂ∫î‰∏éP/D disaggregationËÑöÈ™®ÁöÑ event_loop_ppÂáΩÊï∞ÔºåÂπ∂ÊéíÊü•ÂÖ∂‰ªñÁõ∏ÂÖ≥ÈóÆÈ¢òÔºåÁªôÊàëÂ¢ûÂä†PDÂàÜÁ¶ªÊ®°ÂºèÁöÑpipline Âπ∂Ë°åÂäüËÉΩÔºå‰øùÂÆÅÂÆåÂñÑ



        # PP stage‰πãÈó¥Âêå‰∏Ä‰∏™reqÁöÑkv_receiverÁä∂ÊÄÅÂêåÊ≠•
        if hasattr(self.scheduler, 'pp_group') and self.scheduler.pp_group.world_size > 1:
            self._sync_kv_receiver_states_across_pp_stages(polls)

    def _sync_kv_receiver_states_across_pp_stages(self, polls):
        """
        Âú®PP stage‰πãÈó¥ÂêåÊ≠•kv_receiverÁä∂ÊÄÅÔºå‰ΩøÁî®all_reduce MINÊìç‰ΩúÁ°Æ‰øùÁä∂ÊÄÅ‰∏ÄËá¥ÊÄß
        Á±ª‰ºº‰∫é dist.all_reduce(tensor_to_reduce, op=dist.ReduceOp.MIN, group=pp_group)
        """
        try:
            current_rank = self.scheduler.pp_group.rank_in_group
            pp_world_size = self.scheduler.pp_group.world_size
            
            if pp_world_size <= 1:
                return  # Âè™Êúâ‰∏Ä‰∏™PP stageÔºåÊó†ÈúÄÂêåÊ≠•
            
            # ÊûÑÂª∫request IDÂà∞Êú¨Âú∞ÈòüÂàóÁ¥¢ÂºïÁöÑÊò†Â∞Ñ
            req_id_to_local_idx = {}
            for i, decode_req in enumerate(self.queue):
                req_id_to_local_idx[decode_req.req.rid] = i
            
            # Êî∂ÈõÜÊâÄÊúâPP stagesÁöÑkv_receiverÁä∂ÊÄÅ
            all_stages_states = self._collect_all_pp_stages_states(polls)
            
            # ÂØπ‰∫éÊØè‰∏™ËØ∑Ê±ÇÔºå‰ΩøÁî®MINÊìç‰ΩúÂêåÊ≠•Áä∂ÊÄÅ
            sync_updates = self._apply_min_reduce_logic(all_stages_states, req_id_to_local_idx, polls)
            
            # ËæìÂá∫ÂêåÊ≠•Êó•Âøó
            if sync_updates:
                logger.info(
                    f"PP Stage {current_rank} kv_receiver sync updates:\n" +
                    "\n".join(sync_updates)
                )
            else:
                logger.debug(f"PP Stage {current_rank} kv_receiver states already consistent")
            
            # ÊúÄÁªàbarrierÂêåÊ≠•ÔºåÁ°Æ‰øùÊâÄÊúâstagesÂÆåÊàêÁä∂ÊÄÅÂêåÊ≠•
            self.scheduler.pp_group.barrier()
            
        except Exception as e:
            logger.warning(f"PP kv_receiver state sync failed on stage {current_rank}: {e}")
            # ÂêåÊ≠•Â§±Ë¥•Êó∂ÁªßÁª≠ÊâßË°åÔºåÈÅøÂÖçÈòªÂ°ûÊï¥‰∏™ÊµÅÁ®ã
            pass

    def _collect_all_pp_stages_states(self, polls):
        """Êî∂ÈõÜÊâÄÊúâPP stagesÁöÑkv_receiverÁä∂ÊÄÅ"""
        current_rank = self.scheduler.pp_group.rank_in_group
        pp_world_size = self.scheduler.pp_group.world_size
        
        # ÊûÑÂª∫ÂΩìÂâçstageÁöÑÁä∂ÊÄÅ
        current_stage_requests = {}
        for i, (decode_req, poll) in enumerate(zip(self.queue, polls)):
            current_stage_requests[decode_req.req.rid] = {
                "poll_status": int(poll),
                "bootstrap_room": decode_req.req.bootstrap_room,
                "local_idx": i
            }
        
        # Êî∂ÈõÜÊâÄÊúâstagesÁöÑÁä∂ÊÄÅ
        all_stages_states = [None] * pp_world_size
        all_stages_states[current_rank] = current_stage_requests
        
        # ‰∏éÂÖ∂‰ªñstages‰∫§Êç¢Áä∂ÊÄÅ‰ø°ÊÅØ
        for other_rank in range(pp_world_size):
            if other_rank == current_rank:
                continue
                
            # ‰ΩøÁî®rankÈ°∫Â∫èÈÅøÂÖçÊ≠ªÈîÅ
            if current_rank < other_rank:
                self.scheduler.pp_group.send_object(current_stage_requests, dst=other_rank)
                other_stage_requests = self.scheduler.pp_group.recv_object(src=other_rank)
            else:
                other_stage_requests = self.scheduler.pp_group.recv_object(src=other_rank)
                self.scheduler.pp_group.send_object(current_stage_requests, dst=other_rank)
            
            all_stages_states[other_rank] = other_stage_requests
        
        return all_stages_states

    def _apply_min_reduce_logic(self, all_stages_states, req_id_to_local_idx, polls):
        """
        Â∫îÁî®MIN reduceÈÄªËæëÔºåÂØπ‰∫éÁõ∏ÂêåÁöÑkv_receiverÂèñÊúÄÂ∞èÁä∂ÊÄÅÂÄº
        Á±ª‰ºº‰∫é: tensor_to_reduce = min(tensor_to_reduce) across all PP stages
        """
        current_rank = self.scheduler.pp_group.rank_in_group
        sync_updates = []
        
        # Êî∂ÈõÜÊâÄÊúâstages‰∏≠Âá∫Áé∞ÁöÑËØ∑Ê±ÇID
        all_req_ids = set()
        for stage_requests in all_stages_states:
            if stage_requests is not None:
                all_req_ids.update(stage_requests.keys())
        
        # ÂØπÊØè‰∏™ËØ∑Ê±ÇIDËøõË°åMIN reduceÊìç‰Ωú
        for req_id in all_req_ids:
            if req_id not in req_id_to_local_idx:
                continue  # ÂΩìÂâçstageÊ≤°ÊúâËøô‰∏™ËØ∑Ê±ÇÔºåË∑≥Ëøá
                
            local_idx = req_id_to_local_idx[req_id]
            current_status = int(polls[local_idx])
            
            # Êî∂ÈõÜÊâÄÊúâstages‰∏≠Ëøô‰∏™ËØ∑Ê±ÇÁöÑÁä∂ÊÄÅ
            req_statuses = []
            stages_with_req = []
            
            for stage_rank, stage_requests in enumerate(all_stages_states):
                if stage_requests is not None and req_id in stage_requests:
                    req_statuses.append(stage_requests[req_id]["poll_status"])
                    stages_with_req.append(stage_rank)
            
            if len(req_statuses) > 1:
                # ‰ΩøÁî®MINÊìç‰ΩúÊâæÂà∞ÊúÄÂ∞èÁä∂ÊÄÅÂÄºÔºàÊúÄ‰øùÂÆàÁöÑÁä∂ÊÄÅÔºâ
                min_status = min(req_statuses)
                
                if min_status != current_status:
                    # Êõ¥Êñ∞Êú¨Âú∞Áä∂ÊÄÅ‰∏∫ÊúÄÂ∞èÂÄº
                    polls[local_idx] = min_status
                    sync_updates.append(
                        f"Req {req_id}: MIN reduce updated poll_status from {current_status} "
                        f"to {min_status} (stages: {stages_with_req}, statuses: {req_statuses})"
                    )
                else:
                    logger.debug(
                        f"Req {req_id}: Already at MIN status {current_status} "
                        f"(stages: {stages_with_req}, statuses: {req_statuses})"
                    )
        
        return sync_updates

# DeepSeek 32B Â§öÊú∫ÈÉ®ÁΩ≤ÈÖçÁΩÆ
# ÈúÄË¶ÅËá≥Â∞ë2Âè∞8Âç°Êú∫Âô®
# ‰ΩøÁî® hostNetwork Áõ¥Êé•ÈÄöËøáÂÆø‰∏ªÊú∫IPËÆøÈóÆÔºåÊó†ÈúÄService

---
# Prefill Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek32b-prefill
  labels:
    app: deepseek32b-prefill
    yice: "true"
    environment: test
    release: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: deepseek32b-prefill
      role: prefill
  template:
    metadata:
      labels:
        app: deepseek32b-prefill
        role: prefill
        component: prefill
        yice: "true"
        environment: test
        release: test
    spec:
      containers:
      - name: sglang-prefill
        image: aicr.byd.com/docker.io/lmsysorg/sglang:v0.4.7-cu124-post1
        command:
        - python3
        - -m
        - sglang.launch_server
        - --port
        - "30000"
        - --host
        - "0.0.0.0"
        - --model-path
        - /models/DeepSeek-R1-Distill-Qwen-32B
        - --page-size
        - "64"
        - --disaggregation-mode
        - prefill
        - --mem-fraction-static
        - "0.85"
        - --tp-size
        - "8"  # ‰ΩøÁî®ÂÖ®ÈÉ®8Âº†GPU
        - --disaggregation-ib-device
        - mlx5_bond_0
        - --trust-remote-code
        - --quantization
        - fp8
        - --kv-cache-dtype
        - fp8_e5m2
        - --attention-backend
        - flashinfer
        env:
        - name: NVSHMEM_HCA_PE_MAPPING
          value: "mlx5_bond_0:1:2"
        - name: NVSHMEM_IB_GID_INDEX
          value: "3"
        - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
          value: "1"
        - name: SGLANG_SET_CPU_AFFINITY
          value: "true"
        - name: SGL_ENABLE_JIT_DEEPGEMM
          value: "1"
        - name: NCCL_IB_QPS_PER_CONNECTION
          value: "8"
        - name: NCCL_IB_SPLIT_DATA_ON_QPS
          value: "1"
        - name: NCCL_NET_PLUGIN
          value: none
        - name: NCCL_IB_TC
          value: "136"
        - name: NCCL_MIN_NCHANNELS
          value: "4"
        - name: MC_TE_METRIC
          value: "false"
        - name: NCCL_IB_SL
          value: "5"
        - name: NCCL_IB_HCA
          value: "mlx5_bond_0"
        - name: NCCL_SOCKET_IFNAME
          value: "bond1"
        readinessProbe:
          periodSeconds: 30
          tcpSocket:
            port: 30000
        resources:
          limits:
            nvidia.com/gpu: "8"  # ‰ΩøÁî®ÂÖ®ÈÉ®8Âº†GPU
        securityContext:
          capabilities:
            add:
            - IPC_LOCK
          privileged: true
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /models
          name: host-models
        - mountPath: /dev/infiniband
          name: ib

      # ‰ΩøÁî® affinity ÈÄâÊã©ÂåπÈÖçÁöÑÊú∫Âô®
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: org
                operator: In
                values:
                - "yiceai"
              - key: yiceai
                operator: In
                values:
                - "true"
              - key: deploy
                operator: In
                values:
                - "deepseekr1-32b-pd-p"
        # Âèç‰∫≤ÂíåÊÄßÁ°Æ‰øù prefill Âíå decode ‰∏çÂú®Âêå‰∏ÄËäÇÁÇπ
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - deepseek32b-decode
            topologyKey: kubernetes.io/hostname
      volumes:
      - emptyDir:
          medium: Memory
        name: dshm
      - hostPath:
          path: /export/models
        name: host-models
      - hostPath:
          path: /dev/infiniband
        name: ib
      dnsPolicy: Default
      hostIPC: true
      hostNetwork: true  # ‰ΩøÁî®ÂÆø‰∏ªÊú∫ÁΩëÁªú

---
# Decode Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek32b-decode
  labels:
    app: deepseek32b-decode
    yice: "true"
    environment: test
    release: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: deepseek32b-decode
      role: decode
  template:
    metadata:
      labels:
        app: deepseek32b-decode
        role: decode
        component: decode
        yice: "true"
        environment: test
        release: test
    spec:
      containers:
      - name: sglang-decode
        image: aicr.byd.com/docker.io/lmsysorg/sglang:v0.4.7-cu124-post1
        command:
        - python3
        - -m
        - sglang.launch_server
        - --port
        - "30001"
        - --host
        - "0.0.0.0"
        - --model-path
        - /models/DeepSeek-R1-Distill-Qwen-32B
        - --page-size
        - "64"
        - --disaggregation-mode
        - decode
        - --mem-fraction-static
        - "0.85"
        - --tp-size
        - "8"  # ‰ΩøÁî®ÂÖ®ÈÉ®8Âº†GPU
        - --disaggregation-ib-device
        - mlx5_bond_0
        - --trust-remote-code
        - --quantization
        - fp8
        - --kv-cache-dtype
        - fp8_e5m2
        - --attention-backend
        - flashinfer
        env:
        - name: NVSHMEM_HCA_PE_MAPPING
          value: "mlx5_bond_0:1:2"
        - name: NVSHMEM_IB_GID_INDEX
          value: "3"
        - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
          value: "1"
        - name: SGLANG_SET_CPU_AFFINITY
          value: "true"
        - name: SGL_ENABLE_JIT_DEEPGEMM
          value: "1"
        - name: NCCL_IB_QPS_PER_CONNECTION
          value: "8"
        - name: NCCL_IB_SPLIT_DATA_ON_QPS
          value: "1"
        - name: NCCL_NET_PLUGIN
          value: none
        - name: NCCL_IB_TC
          value: "136"
        - name: NCCL_MIN_NCHANNELS
          value: "4"
        - name: MC_TE_METRIC
          value: "false"
        - name: NCCL_IB_SL
          value: "5"
        - name: NCCL_IB_HCA
          value: "mlx5_bond_0"
        - name: NCCL_SOCKET_IFNAME
          value: "bond1"
        readinessProbe:
          periodSeconds: 30
          tcpSocket:
            port: 30001
        resources:
          limits:
            nvidia.com/gpu: "8"  # ‰ΩøÁî®ÂÖ®ÈÉ®8Âº†GPU
        securityContext:
          capabilities:
            add:
            - IPC_LOCK
          privileged: true
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /models
          name: host-models
        - mountPath: /dev/infiniband
          name: ib

      # ‰ΩøÁî® affinity ÈÄâÊã©ÂåπÈÖçÁöÑÊú∫Âô®
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: org
                operator: In
                values:
                - "yiceai"
              - key: yiceai
                operator: In
                values:
                - "true"
              - key: deploy
                operator: In
                values:
                - "deepseekr1-32b-pd-p"
        # Âèç‰∫≤ÂíåÊÄßÁ°Æ‰øù prefill Âíå decode ‰∏çÂú®Âêå‰∏ÄËäÇÁÇπ
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - deepseek32b-prefill
            topologyKey: kubernetes.io/hostname
      volumes:
      - emptyDir:
          medium: Memory
        name: dshm
      - hostPath:
          path: /export/models
        name: host-models
      - hostPath:
          path: /dev/infiniband
        name: ib
      dnsPolicy: Default
      hostIPC: true
      hostNetwork: true  # ‰ΩøÁî®ÂÆø‰∏ªÊú∫ÁΩëÁªú

---
# Load Balancer Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek32b-loadbalancer
  labels:
    app: deepseek32b-loadbalancer
    yice: "true"
    environment: test
    release: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: deepseek32b-loadbalancer
      role: loadbalancer
  template:
    metadata:
      labels:
        app: deepseek32b-loadbalancer
        role: loadbalancer
        component: loadbalancer
        yice: "true"
        environment: test
        release: test
    spec:
      containers:
      - name: sgl-loadbalancer
        image: lmsysorg/sglang:latest
        command:
        - python
        - -m
        - sglang.srt.disaggregation.mini_lb
        - --prefill
        - $(PREFILL_HOST_URL)
        - --decode
        - $(DECODE_HOST_URL)
        - --host
        - 0.0.0.0
        - --port
        - "8000"
        env:
        - name: PREFILL_HOST_URL
          value: "http://192.168.1.100:30000"  # ÊõøÊç¢‰∏∫ÂÆûÈôÖÁöÑ prefill ÂÆø‰∏ªÊú∫IP
        - name: DECODE_HOST_URL
          value: "http://192.168.1.101:30001"  # ÊõøÊç¢‰∏∫ÂÆûÈôÖÁöÑ decode ÂÆø‰∏ªÊú∫IP
        readinessProbe:
          periodSeconds: 30
          tcpSocket:
            port: 8000
        resources:
          limits:
            cpu: "2"
            memory: "4Gi"

      # ‰ΩøÁî® affinity ÈÄâÊã©ÂåπÈÖçÁöÑÊú∫Âô®ÔºåÂª∫ËÆÆ‰∏é prefill/decode ÂàÜÁ¶ª
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: org
                operator: In
                values:
                - "yiceai"
              - key: yiceai
                operator: In
                values:
                - "true"
              - key: deploy
                operator: In
                values:
                - "deepseekr1-32b-pd-p"
        # Âèç‰∫≤ÂíåÊÄßÁ°Æ‰øù LB ‰∏ç‰∏é prefill/decode Âú®Âêå‰∏ÄËäÇÁÇπÔºàÂèØÈÄâÔºâ
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - deepseek32b-prefill
                  - deepseek32b-decode
              topologyKey: kubernetes.io/hostname
      hostNetwork: true  # ‰ΩøÁî®ÂÆø‰∏ªÊú∫ÁΩëÁªú
      dnsPolicy: Default


#!/bin/bash

set -e  # ÈÅáÂà∞ÈîôËØØÊó∂ÈÄÄÂá∫ËÑöÊú¨

# ÂÆâË£Ö git Âíå git-lfsÔºàÈÄÇÈÖç Debian/UbuntuÔºåÂÖ∂‰ªñÁ≥ªÁªüÈúÄÊâãÂä®ÊõøÊç¢Ôºâ
if ! command -v git &> /dev/null; then
    echo "ÂÆâË£Ö git..."
    sudo apt-get update
    sudo apt-get install -y git
else
    echo "git Â∑≤ÂÆâË£Ö"
fi

if ! command -v git-lfs &> /dev/null; then
    echo "ÂÆâË£Ö git-lfs..."
    sudo apt-get install -y git-lfs
else
    echo "git-lfs Â∑≤ÂÆâË£Ö"
fi

# ÊøÄÊ¥ª git-lfs
echo "ÊøÄÊ¥ª git-lfs..."
git lfs install

# ÂàõÂª∫Âπ∂ËøõÂÖ•ÁõÆÊ†áÁõÆÂΩï
mkdir -p ms-models
cd ms-models

# Clone Ê®°Âûã‰ªìÂ∫ì
echo "Cloning Qwen3-235B-A22B-Instruct-2507-FP8..."
git clone https://www.modelscope.cn/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8.git

echo "Cloning Qwen3-Coder-480B-A35B-Instruct-FP8..."
git clone https://www.modelscope.cn/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8.git

MOE
https://pan.baidu.com/s/1gcjUMDz_vcmLS6yZAeH35Q?pwd=pbo8
https://zhuanlan.zhihu.com/p/719466709
https://triton.csdn.net/6719f57fcd8b2677c3d4ab94.html
https://zhuanlan.zhihu.com/p/21251657579
https://zhuanlan.zhihu.com/p/21251657579
https://zhuanlan.zhihu.com/p/25401744621
https://zhuanlan.zhihu.com/p/1895178845830771205
https://blog.csdn.net/zpp13hao1/article/details/147891337
https://zhuanlan.zhihu.com/p/1911059432953061899
https://mmssai.com/archives/33850
https://www.cnblogs.com/cavalier-chen/p/18937098
https://zhuanlan.zhihu.com/p/26436168971
https://zhuanlan.zhihu.com/p/1895178845830771205
https://blog.csdn.net/zpp13hao1/article/details/147891337
https://mmssai.com/archives/33850
echo "ÊâÄÊúâÊìç‰ΩúÂÆåÊàêÔºÅ"



nvshmem
https://docs.nvidia.com/nvshmem/release-notes-install-guide/install-guide/abstract.html
https://zhuanlan.zhihu.com/p/26575363781
https://blog.csdn.net/jkjgj/article/details/142066495
https://zhuanlan.zhihu.com/p/1952325639211315297
https://zhuanlan.zhihu.com/p/1941562551407187752
https://zhuanlan.zhihu.com/p/1933899894881489690
https://blog.csdn.net/jkjgj/article/details/142066495
https://zhuanlan.zhihu.com/p/26082845081
https://www.51cto.com/aigc/6550.html
https://blog.csdn.net/gitblog_00341/article/details/151640319



__device__ __forceinline__ float gelu(const float& val) {
  constexpr float kAlpha = M_SQRT1_2;
  return val * 0.5f * (1.0f + ::erf(val * kAlpha));
}

__device__ __forceinline__ float gelu_tanh(const float& val) {
  const float cdf =
      0.5f * (1.0f + math::tanh((0.7978845608028654f * (val + 0.044715f * val * val * val))));
  return val * cdf;
}

void silu_and_mul(at::Tensor& out, at::Tensor& input, bool enable_pdl) {
  int d = input.size(-1) / 2;
  int64_t num_tokens = input.numel() / input.size(-1);

  const c10::cuda::OptionalCUDAGuard device_guard(out.device());
  auto stream = at::cuda::getCurrentCUDAStream();

  DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
    uint32_t vec_size = 16 / sizeof(c_type);
    cudaLaunchConfig_t config;
    config.gridDim = num_tokens;
    config.blockDim = std::min(d / vec_size, 1024U);
    config.dynamicSmemBytes = 0;
    config.stream = stream;
    cudaLaunchAttribute attrs[1];
    attrs[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;
    attrs[0].val.programmaticStreamSerializationAllowed = enable_pdl;
    config.numAttrs = 1;
    config.attrs = attrs;
    auto kernel = flashinfer::activation::act_and_mul_kernel<c_type, silu>;

    cudaLaunchKernelEx(&config, kernel, static_cast<c_type*>(out.data_ptr()),
                       static_cast<c_type*>(input.data_ptr()), d);

namespace activation {
template <typename T, float (*Activation)(const float&)>
__global__ void act_and_mul_kernel(T* __restrict__ out, const T* __restrict__ input, const int d) {
  constexpr uint32_t vec_size = 16 / sizeof(T);
  const int64_t token_idx = blockIdx.x;
  const int64_t thread_idx = threadIdx.x;
  const int64_t stride = blockDim.x;
  const int64_t offset = token_idx * 2 * d;

#if (__CUDACC_VER_MAJOR__ >= 12 && defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))
  asm volatile("griddepcontrol.wait;");
#endif

#pragma unroll 1
  for (uint32_t idx = thread_idx; idx < d / vec_size; idx += stride) {
    vec_t<float, vec_size> x_vec, y_vec, out_vec;
    x_vec.cast_load(input + offset + idx * vec_size);
    y_vec.cast_load(input + offset + d + idx * vec_size);
#pragma unroll
    for (uint32_t i = 0; i < vec_size; ++i) {
      out_vec[i] = Activation(x_vec[i]) * y_vec[i];
    }
    out_vec.cast_store(out + token_idx * d + idx * vec_size);
  }

  const int64_t remaining_offset = d - d % (stride * vec_size);
  // process the remaining elements
#pragma unroll 1
  for (int64_t idx = thread_idx; idx < d % (stride * vec_size); idx += stride) {
    float x = input[offset + remaining_offset + idx],
          y = input[offset + remaining_offset + d + idx];
    out[token_idx * d + remaining_offset + idx] = Activation(x) * y;
  }

#if (__CUDACC_VER_MAJOR__ >= 12 && defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))
  asm volatile("griddepcontrol.launch_dependents;");
#endif
}

#include <algorithm>
#include <cuda_fp16.h>
#include <cuda_runtime.h>
#include <float.h>
#include <stdio.h>
#include <stdlib.h>
#include <torch/extension.h>
#include <torch/types.h>
#include <vector>

__device__ __forceinline__ __nv_bfloat162 silu_mul_half2(const __nv_bfloat162& val, const __nv_bfloat162& factor ) {
      return __hmul2(__h2div(val, __hadd2(__float2bfloat162_rn(1.0f), h2exp(__hneg2(val)))), factor);
}
__global__ void act_mul_f32_kernel(float* __restrict__ out, const float* __restrict__ input, const int d) {
        const int64_t vec_size = 4;
    const int64_t token_idx = blockIdx.x;
    const int64_t thread_idx = threadIdx.x;
    const int64_t stride = blockDim.x;
    const int64_t offset = token_idx * 2 * d;

    cudaDeviceSynchronize(); 
    cudaError_t err = cudaGetLastError(); 
    if (err != cudaSuccess) { 
        printf("CUDA Error: %s\n",cudaGetErrorString(err)); 
    }







def convert_compressed_tensor_to_gptq(
    weight_packed: torch.Tensor,
    weight_scale: torch.Tensor,
    weight_shape: list[int],
    group_size: int = 32,
) -> dict[str, torch.Tensor]:
    """
    Convert compressed-tensors format to GPTQ format for TRT-LLM.
    Args:
        weight_packed: Shape (N, K/8) dtype int32
        weight_scale: Shape (N, K/group_size) dtype bfloat16
        weight_shape: [N, K] - original weight shape
        group_size: Quantization group size
    Returns:
        Dictionary with:
            - qweight: Shape (K/8, N) dtype int32 (transposed!)
            - scales: Shape (K/group_size, N) dtype fp16 (transposed!)
            - qzeros: Shape (K/group_size, N/8) dtype int32 (for symmetric, all zeros)
    """
    n, k = weight_shape

    # TRT-LLM expects weights transposed: (K, N) instead of (N, K)
    # But packed format keeps the packing dimension, so:
    # Source: (N, K/8) -> Target: (K/8, N)
    qweight = weight_packed.t().contiguous()

    # Scales also need transpose: (N, K/group_size) -> (K/group_size, N)
    scales = weight_scale.t().contiguous().to(torch.float16)

    # For symmetric quantization, we use zero as the zero-point
    # GPTQ format expects qzeros in packed format
    # Shape: (K/group_size, N/8) since zeros are also packed 8 per int32
    num_groups = k // group_size
    # Create zeros tensor - for symmetric quantization, zero-point is 8 (middle of [0,15])
    # Pack as uint4: each int32 contains 8 nibbles of value 8
    # Create as bytes first then view as int32
    qzeros_uint8 = torch.full((num_groups, n // 2), 0x88, dtype=torch.uint8)
    qzeros = qzeros_uint8.view(torch.int32).contiguous()

    return {
        "qweight": qweight,
        "scales": scales,
        "qzeros": qzeros,
    }


def convert_checkpoint(
    input_dir: str,
    output_dir: str,
    num_shards: int | None = None,
    skip_existing: bool = True,
):
    """
    Convert all shards from compressed-tensors to GPTQ format.
    Args:
        input_dir: Source checkpoint directory
        output_dir: Output checkpoint directory
        num_shards: Number of shards to process (None = all)
        skip_existing: Skip conversion if output shard already exists
    """
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Find all safetensors files
    shard_files = sorted(input_path.glob("model-*.safetensors"))
    if not shard_files:
        raise ValueError(f"No model shards found in {input_dir}")

    if num_shards is not None:
        shard_files = shard_files[:num_shards]

    print(f"Found {len(shard_files)} shards to process")

    # Track weight mapping for index file
    new_weight_map = {}

    # Load and convert each shard
    for shard_idx, shard_file in enumerate(tqdm(shard_files, desc="Processing shards")):
        shard_name = shard_file.name
        output_file = output_path / shard_name

        # Check if output already exists
        if skip_existing and output_file.exists():
            print(f"\n‚è≠Ô∏è  Skipping {shard_name} (already exists)")
            # Still need to build the weight_map from existing file
            with safetensors.safe_open(str(output_file), framework="pt", device="cpu") as f:
                for key in f:
                    new_weight_map[key] = shard_name
            continue

        print(f"\nüîÑ Converting {shard_file.name}...")

        # Load source shard
        source_tensors = {}
        with safetensors.safe_open(str(shard_file), framework="pt", device="cpu") as f:
            for key in f:
                source_tensors[key] = f.get_tensor(key)

        # Convert tensors
        output_tensors = {}

        for key, tensor in tqdm(source_tensors.items(), desc="Converting tensors", leave=False):
            if key.endswith(".weight_packed"):
                # This is a quantized weight - convert to GPTQ format
                base_key = key[: -len(".weight_packed")]
                scale_key = base_key + ".weight_scale"
                shape_key = base_key + ".weight_shape"

                if scale_key in source_tensors and shape_key in source_tensors:
                    weight_shape = source_tensors[shape_key].tolist()

                    # Convert to GPTQ format
                    gptq_tensors = convert_compressed_tensor_to_gptq(
                        weight_packed=tensor,
                        weight_scale=source_tensors[scale_key],
                        weight_shape=weight_shape,
                        group_size=32,
                    )

                    # Save with GPTQ naming convention and track in weight_map
                    qweight_key = base_key + ".qweight"
                    scales_key = base_key + ".scales"
                    qzeros_key = base_key + ".qzeros"

                    output_tensors[qweight_key] = gptq_tensors["qweight"]
                    output_tensors[scales_key] = gptq_tensors["scales"]
                    output_tensors[qzeros_key] = gptq_tensors["qzeros"]

                    new_weight_map[qweight_key] = shard_name
                    new_weight_map[scales_key] = shard_name
                    new_weight_map[qzeros_key] = shard_name
                else:
                    print(f"Warning: Missing scale or shape for {key}")

            elif key.endswith((".weight_scale", ".weight_shape")):
                # Skip these as they're handled above
                continue
            else:
                # Keep non-quantized tensors as-is
                output_tensors[key] = tensor
                new_weight_map[key] = shard_name

        # Save converted shard
        safetensors.torch.save_file(output_tensors, str(output_file))
        print(f"‚úÖ Saved to {output_file}")

    # Copy config.json and update quantization settings
    config_file = input_path / "config.json"
    if config_file.exists():
        with open(config_file) as f:
            config = json.load(f)

        # Remove HuggingFace quantization_config if present
        config.pop("quantization_config", None)

        # Add TRT-LLM native quantization config for GPTQ
        config["quantization"] = {
            "quant_algo": "W4A16_GPTQ",  # TRT-LLM's enum value
            "group_size": 32,
            "has_zero_point": True,  # GPTQ uses asymmetric quantization
            "pre_quant_scale": False,  # No pre-quantization scaling
        }

        output_config_file = output_path / "config.json"
        with open(output_config_file, "w") as f:
            json.dump(config, f, indent=2)
        print(f"\nSaved config to {output_config_file}")

    # Generate new safetensors index file
    index_data = {
        "metadata": {
            "total_size": sum(
                (output_path / shard_file.name).stat().st_size for shard_file in shard_files
            )
        },
        "weight_map": new_weight_map,
    }

    index_file = output_path / "model.safetensors.index.json"
    with open(index_file, "w") as f:
        json.dump(index_data, f, indent=2)
    print(f"\nGenerated index file: {index_file}")
    print(f"  Total tensors: {len(new_weight_map)}")

    # Copy other necessary files
    import shutil

    # JSON files (tokenizer and generation config)
    for file in [
        "generation_config.json",
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
        "vocab.json",
        "merges.txt",
    ]:
        src = input_path / file
        if src.exists():
            shutil.copy(src, output_path / file)
            print(f"Copied: {file}")

    # Python files (model architecture, custom tokenizers)
    for file in ["configuration_deepseek.py", "modeling_deepseek.py", "tokenization_kimi.py"]:
        src = input_path / file
        if src.exists():
            shutil.copy(src, output_path / file)
            print(f"Copied: {file}")

    # Tokenizer model files
    for file in ["tiktoken.model", "tokenizer.model", "sentencepiece.model"]:
        src = input_path / file
        if src.exists():
            shutil.copy(src, output_path / file)
            print(f"Copied: {file}")

    # Template files
    for file in ["chat_template.jinja", "chat_template.json"]:
        src = input_path / file
        if src.exists():
            shutil.copy(src, output_path / file)
            print(f"Copied: {file}")

    print(f"\n‚úì Conversion complete! Output saved to {output_dir}")


def main():
    parser = argparse.ArgumentParser(
        description="Convert Kimi-K2 checkpoint to TRT-LLM GPTQ format"
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        default="/home/scratch.omniml_data_1/models/Kimi-K2-Thinking",
        help="Input checkpoint directory with compressed-tensors format",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="/home/scratch.omniml_data_2/zhiyuc/checkpoints/Kimi-K2-Thinking-GPTQ",
        help="Output directory for GPTQ format checkpoint",
    )
    parser.add_argument(
        "--num-shards",
        type=int,
        default=None,
        help="Number of shards to convert (default: all)",
    )
    parser.add_argument(
        "--no-skip-existing",
        action="store_true",
        help="Re-convert shards even if they already exist (default: skip existing)",
    )

    args = parser.parse_args()

    convert_checkpoint(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        num_shards=args.num_shards,
        skip_existing=not args.no_skip_existing,
    )


if __name__ == "__main__":
    main()
